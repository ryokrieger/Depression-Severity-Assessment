{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5910afcb",
   "metadata": {},
   "source": [
    "# Model Training (NLP)\n",
    "\n",
    "This notebook trains 5 transformer-based NLP models:\n",
    "- BERT\n",
    "- ClinicalBERT\n",
    "- DistilBERT\n",
    "- BioBERT\n",
    "- ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "NLP_FEATURE_DIR = Path(\"../data/processed/nlpfeatures2\")\n",
    "PROC_OUT = Path(\"../data/processed/nlp2\")\n",
    "MODEL_OUT = Path(\"../models/nlp2\")\n",
    "FIG_OUT = Path(\"../figures/nlp2\")\n",
    "\n",
    "for p in [PROC_OUT, MODEL_OUT, FIG_OUT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_EPOCHS = 5 \n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "MAX_LENGTH = 128\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ac5ed",
   "metadata": {},
   "source": [
    "## Load the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef03b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = NLP_FEATURE_DIR / \"train.csv\"\n",
    "test_csv  = NLP_FEATURE_DIR / \"test.csv\"\n",
    "\n",
    "if not train_csv.exists() or not test_csv.exists():\n",
    "    raise FileNotFoundError(f\"Train/test CSVs not found in {NLP_FEATURE_DIR}. Please run 20_feature_engineering_nlp.ipynb first.\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df  = pd.read_csv(test_csv)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nSample:\")\n",
    "display(train_df.head(2))\n",
    "\n",
    "for c in [\"Student Information\", \"Depression Label\"]:\n",
    "    if c not in train_df.columns:\n",
    "        raise ValueError(f\"Column {c} missing in {train_csv}\")\n",
    "    if c not in test_df.columns:\n",
    "        raise ValueError(f\"Column {c} missing in {test_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b842b5",
   "metadata": {},
   "source": [
    "## Encode target labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_df[\"label_enc\"] = le.fit_transform(train_df[\"Depression Label\"].astype(str))\n",
    "test_df[\"label_enc\"]  = le.transform(test_df[\"Depression Label\"].astype(str))\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n",
    "print(\"Num labels:\", num_labels)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"Student Information\", \"label_enc\"]].rename(columns={\"Student Information\":\"text\", \"label_enc\":\"label\"}))\n",
    "test_ds  = Dataset.from_pandas(test_df[[\"Student Information\", \"label_enc\"]].rename(columns={\"Student Information\":\"text\", \"label_enc\":\"label\"}))\n",
    "\n",
    "print(train_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf622a",
   "metadata": {},
   "source": [
    "## Models list and tokenizer/model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    (\"bert-base-uncased\", \"bert-base-uncased\"),\n",
    "    (\"clinical-bert\", \"emilyalsentzer/Bio_ClinicalBERT\"),\n",
    "    (\"distilbert-base-uncased\", \"distilbert-base-uncased\"),\n",
    "    (\"biobert-base-cased-v1.1\", \"dmis-lab/biobert-base-cased-v1.1\"),\n",
    "    (\"albert-base-v2\", \"albert-base-v2\"),\n",
    "]\n",
    "\n",
    "print(\"Models to run:\", [m[0] for m in MODELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec66c90",
   "metadata": {},
   "source": [
    "## Define compute_metrics used by Trainer (accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc220065",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODELS = MODELS\n",
    "\n",
    "from transformers import logging as tf_logging\n",
    "tf_logging.set_verbosity_error()\n",
    "\n",
    "for slug, hf_name in RUN_MODELS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Starting training: {slug}  (HF model: {hf_name})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model_out_dir = MODEL_OUT / slug\n",
    "    fig_out_dir = FIG_OUT / slug\n",
    "    proc_out_dir = PROC_OUT\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    proc_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_name, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(hf_name, num_labels=num_labels)\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=MAX_LENGTH)\n",
    "\n",
    "    tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized_test  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(model_out_dir),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=False,\n",
    "        push_to_hub=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Beginning training...\")\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(str(model_out_dir))\n",
    "    print(\"Model saved to:\", model_out_dir)\n",
    "\n",
    "    print(\"Running final evaluation on test set...\")\n",
    "    metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "    print(\"Final metrics:\", metrics)\n",
    "\n",
    "    logs = trainer.state.log_history\n",
    "    epoch_nums = []\n",
    "    epoch_acc = []\n",
    "    epoch_prec = []\n",
    "    epoch_rec = []\n",
    "    epoch_f1 = []\n",
    "    for entry in logs:\n",
    "        if \"eval_accuracy\" in entry:\n",
    "            epoch_nums.append(entry.get(\"epoch\"))\n",
    "            epoch_acc.append(entry.get(\"eval_accuracy\"))\n",
    "            epoch_prec.append(entry.get(\"eval_precision\"))\n",
    "            epoch_rec.append(entry.get(\"eval_recall\"))\n",
    "            epoch_f1.append(entry.get(\"eval_f1\"))\n",
    "    if not epoch_nums:\n",
    "        epoch_nums = [i+1 for i in range(NUM_EPOCHS)]\n",
    "        epoch_acc = [metrics.get(\"eval_accuracy\")] * NUM_EPOCHS\n",
    "        epoch_prec = [metrics.get(\"eval_precision\")] * NUM_EPOCHS\n",
    "        epoch_rec = [metrics.get(\"eval_recall\")] * NUM_EPOCHS\n",
    "        epoch_f1 = [metrics.get(\"eval_f1\")] * NUM_EPOCHS\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"epoch\": epoch_nums,\n",
    "        \"accuracy\": epoch_acc,\n",
    "        \"precision\": epoch_prec,\n",
    "        \"recall\": epoch_rec,\n",
    "        \"f1\": epoch_f1\n",
    "    })\n",
    "    results_csv = proc_out_dir / f\"{slug}_results.csv\"\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(\"Saved epoch metrics CSV ->\", results_csv)\n",
    "    \n",
    "    summary_out = proc_out_dir / f\"{slug}_final_metrics.csv\"\n",
    "    pd.DataFrame([metrics]).to_csv(summary_out, index=False)\n",
    "    print(\"Saved final metrics ->\", summary_out)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(epoch_nums, epoch_acc, marker='o', label='Accuracy')\n",
    "    ax.plot(epoch_nums, epoch_f1, marker='o', label='F1')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(f\"{slug} — Accuracy/F1 vs Epoch\")\n",
    "    ax.set_xticks(epoch_nums)\n",
    "    ax.legend()\n",
    "    acc_fig_path = fig_out_dir / f\"{slug}_accuracy_epoch.png\"\n",
    "    fig.savefig(acc_fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved accuracy vs epoch ->\", acc_fig_path)\n",
    "\n",
    "    print(\"Computing confusion matrix on test set...\")\n",
    "    preds_output = trainer.predict(tokenized_test)\n",
    "    pred_labels = np.argmax(preds_output.predictions, axis=-1)\n",
    "    true_labels = preds_output.label_ids\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    cm_fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"{slug} — Confusion Matrix\")\n",
    "    cm_path = fig_out_dir / f\"{slug}_confusion.png\"\n",
    "    cm_fig.savefig(cm_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(cm_fig)\n",
    "    print(\"Saved confusion matrix ->\", cm_path)\n",
    "\n",
    "    mapping_path = model_out_dir / \"label_mapping.csv\"\n",
    "    pd.DataFrame({\"label\": list(le.classes_), \"enc\": list(range(len(le.classes_)))}).to_csv(mapping_path, index=False)\n",
    "    print(\"Saved label mapping ->\", mapping_path)\n",
    "    \n",
    "    print(f\"Finished model: {slug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c885b",
   "metadata": {},
   "source": [
    "## Aggregate per-model final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd34900",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = []\n",
    "for slug, hf_name in MODELS:\n",
    "    summary_out = PROC_OUT / f\"{slug}_final_metrics.csv\"\n",
    "    if summary_out.exists():\n",
    "        dfm = pd.read_csv(summary_out)\n",
    "        dfm[\"model\"] = slug\n",
    "        agg.append(dfm)\n",
    "    else:\n",
    "        print(\"Summary missing for\", slug)\n",
    "\n",
    "if agg:\n",
    "    combined = pd.concat(agg, ignore_index=True, sort=False)\n",
    "    display(combined[[\"model\", \"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\", \"eval_loss\"]])\n",
    "    combined.to_csv(PROC_OUT / \"all_nlp_models_summary.csv\", index=False)\n",
    "    print(\"Saved combined summary ->\", PROC_OUT / \"all_nlp_models_summary.csv\")\n",
    "else:\n",
    "    print(\"No final metrics found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
