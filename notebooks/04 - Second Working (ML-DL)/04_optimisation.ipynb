{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde818b4",
   "metadata": {},
   "source": [
    "# Model Optimisation\n",
    "\n",
    "This notebook performs hyperparameter optimisation for the six classical ML models across the nine feature-sets:\n",
    "- RandomizedSearchCV\n",
    "- GridSearchCV\n",
    "- BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "FEATURES_BASE = Path(\"../data/processed/features1\")\n",
    "PROC_BASE = Path(\"../data/processed/ml1\")\n",
    "MODEL_BASE = Path(\"../models/ml1\")\n",
    "FIG_BASE = Path(\"../figures/ml1\")\n",
    "\n",
    "for p in [PROC_BASE, MODEL_BASE, FIG_BASE]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METHODS = [\"rfe\",\"skb\",\"fscs\",\"etc\",\"pc\",\"mi\",\"mir\",\"mu\",\"vt\"]\n",
    "\n",
    "OPT_MAP = {\n",
    "    \"rfe\": \"orfe\",\n",
    "    \"skb\": \"oskb\",\n",
    "    \"fscs\": \"ofscs\",\n",
    "    \"etc\": \"oetc\",\n",
    "    \"pc\": \"opc\",\n",
    "    \"mi\": \"omi\",\n",
    "    \"mir\": \"omir\",\n",
    "    \"mu\": \"omu\",\n",
    "    \"vt\": \"ovt\"\n",
    "}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CV = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "except Exception:\n",
    "    try:\n",
    "        from skopt import BayesSearchCV\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"skopt (scikit-optimize) not found. Install scikit-optimize to use Bayesian optimisation.\") from e\n",
    "\n",
    "print(\"Configuration OK. Methods:\", METHODS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe49320",
   "metadata": {},
   "source": [
    "## Hyperparameter Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f63685",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DIST = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": uniform(0.001, 10),\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"max_iter\": [500]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": randint(50, 300),\n",
    "        \"learning_rate\": uniform(0.01, 0.5),\n",
    "        \"max_depth\": randint(2, 8),\n",
    "        \"subsample\": uniform(0.5, 0.5)\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": randint(1, 31),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": randint(50, 400),\n",
    "        \"max_depth\": randint(3, 20),\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": randint(1, 20),\n",
    "        \"min_samples_split\": randint(2, 20),\n",
    "        \"min_samples_leaf\": randint(1, 20)\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": uniform(0.01, 100),\n",
    "        \"kernel\": [\"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "PARAM_GRID = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"penalty\": [\"l2\"]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 5]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": [3,5,7,9],\n",
    "        \"weights\": [\"uniform\",\"distance\"],\n",
    "        \"p\": [1,2]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [None, 10, 20]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"gamma\": [\"scale\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "BAYES_SPACE = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": (1e-3, 100.0, \"log-uniform\"),\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"penalty\": [\"l2\"]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": (50, 300),\n",
    "        \"learning_rate\": (0.01, 0.5, \"log-uniform\"),\n",
    "        \"max_depth\": (2, 8)\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": (1, 31),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": (50, 400),\n",
    "        \"max_depth\": (3, 30)\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": (1, 30),\n",
    "        \"min_samples_split\": (2, 50)\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": (1e-2, 100.0, \"log-uniform\"),\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87757d1b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aae9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, out_path, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(title)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def run_optimizer(name, model, param_space, X_train, y_train, optimizer_type=\"random\", n_iter=30):\n",
    "    if optimizer_type == \"random\":\n",
    "        search = RandomizedSearchCV(model, param_distributions=param_space, n_iter=n_iter, cv=CV,\n",
    "                                    scoring=\"accuracy\", n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "    elif optimizer_type == \"grid\":\n",
    "        search = GridSearchCV(model, param_grid=param_space, cv=CV, scoring=\"accuracy\", n_jobs=-1, verbose=0)\n",
    "    elif optimizer_type == \"bayes\":\n",
    "        search = BayesSearchCV(model, search_spaces=param_space, n_iter=n_iter, cv=CV, scoring=\"accuracy\",\n",
    "                               n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer_type\")\n",
    "    start = time.time()\n",
    "    search.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    return search, end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9e1b7",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"SVM\": SVC(probability=True, random_state=RANDOM_STATE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7224f1c",
   "metadata": {},
   "source": [
    "## Main Optimisation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_rand = 30\n",
    "n_iter_bayes = 30\n",
    "\n",
    "for method in tqdm(METHODS, desc=\"Methods\"):\n",
    "    feat_dir = FEATURES_BASE / method\n",
    "    if not feat_dir.exists():\n",
    "        print(f\"⚠️ Missing feature folder for {method}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    train_df = pd.read_csv(feat_dir / \"train.csv\").dropna(subset=[\"DepressionEncoded\"])\n",
    "    test_df  = pd.read_csv(feat_dir / \"test.csv\").dropna(subset=[\"DepressionEncoded\"])\n",
    "    X_train = train_df.drop(columns=[\"DepressionEncoded\"]).values\n",
    "    y_train = train_df[\"DepressionEncoded\"].astype(int).values\n",
    "    X_test  = test_df.drop(columns=[\"DepressionEncoded\"]).values\n",
    "    y_test  = test_df[\"DepressionEncoded\"].astype(int).values\n",
    "\n",
    "    opt_folder = OPT_MAP.get(method, method)\n",
    "    proc_out_base = PROC_BASE / opt_folder\n",
    "    model_out_base = MODEL_BASE / opt_folder\n",
    "    fig_out_base = FIG_BASE / opt_folder\n",
    "    for p in [proc_out_base, model_out_base, fig_out_base]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rand_results = []\n",
    "    grid_results = []\n",
    "    bayes_results = []\n",
    "\n",
    "    for model_name, model in tqdm(MODELS.items(), desc=f\"Models ({method})\", leave=False):\n",
    "        print(f\"\\n=== {method.upper()} :: {model_name} ===\")\n",
    "\n",
    "        pdist = PARAM_DIST.get(model_name, {})\n",
    "        pgrid = PARAM_GRID.get(model_name, {})\n",
    "        bspace = BAYES_SPACE.get(model_name, {})\n",
    "\n",
    "        try:\n",
    "            print(\"-> RandomizedSearchCV (n_iter=\", n_iter_rand, \")\")\n",
    "            search_rand, t_rand = run_optimizer(model_name, model, pdist, X_train, y_train, optimizer_type=\"random\", n_iter=n_iter_rand)\n",
    "            best_rand = search_rand.best_estimator_\n",
    "            y_pred = best_rand.predict(X_test)\n",
    "            metrics_rand = compute_metrics(y_test, y_pred)\n",
    "            metrics_rand.update({\"Method\": method, \"Model\": model_name, \"Optimizer\": \"Randomized\", \"TimeSec\": t_rand})\n",
    "            rand_results.append(metrics_rand)\n",
    "\n",
    "            joblib.dump(best_rand, model_out_base / f\"{model_name.lower().replace(' ','_')}_randomized.pkl\")\n",
    "            save_confusion_matrix(y_test, y_pred, fig_out_base / f\"{model_name.lower().replace(' ','_')}_randomized_confusion.png\",\n",
    "                                  f\"{model_name} Randomized ({method})\")\n",
    "            print(\" Randomized done. Metrics:\", metrics_rand)\n",
    "        except Exception as e:\n",
    "            print(\" RandomizedSearchCV failed for\", model_name, \":\", e)\n",
    "\n",
    "        try:\n",
    "            print(\"-> GridSearchCV\")\n",
    "            if pgrid:\n",
    "                search_grid, t_grid = run_optimizer(model_name, model, pgrid, X_train, y_train, optimizer_type=\"grid\")\n",
    "                best_grid = search_grid.best_estimator_\n",
    "                y_pred = best_grid.predict(X_test)\n",
    "                metrics_grid = compute_metrics(y_test, y_pred)\n",
    "                metrics_grid.update({\"Method\": method, \"Model\": model_name, \"Optimizer\": \"Grid\", \"TimeSec\": t_grid})\n",
    "                grid_results.append(metrics_grid)\n",
    "\n",
    "                joblib.dump(best_grid, model_out_base / f\"{model_name.lower().replace(' ','_')}_grid.pkl\")\n",
    "                save_confusion_matrix(y_test, y_pred, fig_out_base / f\"{model_name.lower().replace(' ','_')}_grid_confusion.png\",\n",
    "                                      f\"{model_name} Grid ({method})\")\n",
    "                print(\" Grid done. Metrics:\", metrics_grid)\n",
    "            else:\n",
    "                print(\"  No grid defined for\", model_name, \" — skipping GridSearch\")\n",
    "        except Exception as e:\n",
    "            print(\" GridSearchCV failed for\", model_name, \":\", e)\n",
    "\n",
    "        try:\n",
    "            print(\"-> BayesSearchCV (n_iter=\", n_iter_bayes, \")\")\n",
    "            search_bayes, t_bayes = run_optimizer(model_name, model, bspace, X_train, y_train, optimizer_type=\"bayes\", n_iter=n_iter_bayes)\n",
    "            best_bayes = search_bayes.best_estimator_\n",
    "            y_pred = best_bayes.predict(X_test)\n",
    "            metrics_bayes = compute_metrics(y_test, y_pred)\n",
    "            metrics_bayes.update({\"Method\": method, \"Model\": model_name, \"Optimizer\": \"Bayes\", \"TimeSec\": t_bayes})\n",
    "            bayes_results.append(metrics_bayes)\n",
    "\n",
    "            joblib.dump(best_bayes, model_out_base / f\"{model_name.lower().replace(' ','_')}_bayes.pkl\")\n",
    "            save_confusion_matrix(y_test, y_pred, fig_out_base / f\"{model_name.lower().replace(' ','_')}_bayes_confusion.png\",\n",
    "                                  f\"{model_name} Bayes ({method})\")\n",
    "            print(\" Bayes done. Metrics:\", metrics_bayes)\n",
    "        except Exception as e:\n",
    "            print(\" BayesSearchCV failed for\", model_name, \":\", e)\n",
    "\n",
    "    if rand_results:\n",
    "        pd.DataFrame(rand_results).to_csv(proc_out_base / \"randomized_search_results.csv\", index=False)\n",
    "    if grid_results:\n",
    "        pd.DataFrame(grid_results).to_csv(proc_out_base / \"grid_search_results.csv\", index=False)\n",
    "    if bayes_results:\n",
    "        pd.DataFrame(bayes_results).to_csv(proc_out_base / \"bayes_search_results.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nSaved results for method: {method} -> {proc_out_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3072e51",
   "metadata": {},
   "source": [
    "## Combine all optimizer results into a single report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all = []\n",
    "for method in METHODS:\n",
    "    opt_folder = OPT_MAP.get(method, method)\n",
    "    proc_out_base = PROC_BASE / opt_folder\n",
    "    for fname in [\"randomized_search_results.csv\", \"grid_search_results.csv\", \"bayes_search_results.csv\"]:\n",
    "        f = proc_out_base / fname\n",
    "        if f.exists():\n",
    "            df = pd.read_csv(f)\n",
    "            df[\"OptFolder\"] = opt_folder\n",
    "            df[\"SourceFile\"] = fname\n",
    "            combined_all.append(df)\n",
    "\n",
    "if combined_all:\n",
    "    combined_df = pd.concat(combined_all, ignore_index=True)\n",
    "    combined_df.to_csv(PROC_BASE / \"all_optimised_results_summary.csv\", index=False)\n",
    "    display(combined_df.sort_values([\"OptFolder\", \"Model\", \"Optimizer\"], ascending=True))\n",
    "    print(\"Combined optimisation summary saved to:\", PROC_BASE / \"all_optimised_results_summary.csv\")\n",
    "else:\n",
    "    print(\"No optimisation results found to combine.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
